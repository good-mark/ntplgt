
            ﻿-------------------□ □------------------------
Побудовано модифікації методів мінімізації функцій багатьох змінних. Введення параметрів та додаткової інформації про властивості функції дозволяє адаптувати процес обчислень. Доведено умови збіжності та отримано оцінки швидкості
Ключові слова: градієнтний метод, мінімізація, метод двоїстих напрямків
□-------------------------------------□
Построены модификации методов минимизации функций многих переменных. Введение параметров и дополнительной информации о свойствах функции позволяет адаптировать процесс вычислений. Доказано условия сходимости и получены оценки скорости
Ключевые слова: градиентный метод, минимизация, метод двойственных направлений
□-------------------------------------□
Some modifications of functions methods minimization is proposed. The introduction of parameters and additional information about the properties of the function allows adapting the process of computing. The conditions for convergence are proved and estimates of speed are obtained
Keywords: gradient method, minimization method, dual directions method -------------------□ □------------------------
УДК 004.942:517.958:519.6
МЕТОДИ ЧИСЕЛЬНОЇ МІНІМІЗАЦІЇ У МОДЕЛЯХ АНАЛІЗУ ДАНИХ
Ю.В. Нікольський
Кандидат фізико-математичних наук, доцент Кафедра "Інформаційні системи та мережі» Інститут комп’ютерних наук та інформаційних технологій Національного університету «Львівська політехніка» вул. С. Бандери, 12, м. Львів, 79013 Контактний тел. (0322) 370-440, 067-935-53-81 E-mail: y_nikol@yahoo.com
1.	Вступ
Широке застосування методів машинного навчання у процесах аналізу даних [1] знов поставило на порядок денний необхідність побудови обчислювальних процедур для мінімізації функцій багатьох змінних. Такі процедури повинні дозволяти отримувати розв’язки з прийнятною швидкістю та бути незалежними від локальних властивостей функцій. Це викликано масовим застосуванням нейронних мереж прямого поширення сигналу, навчання яких здійснюють методом зворотного поширення похибки [2]. В основу цього методу покладено градієнтний метод, який володіє відомими недоліками, пов’язаними із особливостями формування траєкторії спуску для функцій із специфічним розташуванням локальних екстремумів. Використання класичних методів найшвидшого спуску та методів спряжених градієнтів пов’язане із традицією застосування простих обчислювальних процедур, які в той самий час володіють відомими недоліками.
2.	Постановка задачі дослідження
В останні роки багато зусиль прикладено у дослідження нових методів мінімізації, які основані на ідеї генетичного алгоритму [3]. Такі методи не
набули широкого застосування, оскільки володіють низькою швидкістю збіжності та вимагають адаптації кроків мутації та схрещування до конкретної функції. Крім того, в роботах із застосування нейронних мереж широко застосовують метод Левенберга - Маркуар-да [4]. Цей метод володіє складною обчислювальною процедурою, яка вимагає розв’язування допоміжної нетривіальної задачі знаходження розв’язку системи нелінійних рівнянь, яка рівносильна застосуванню методу Ньютона мінімізації функцій багатьох змінних [5]. Пропонується будувати обчислювальні процедури мінімізації, які можна регулювати за рахунок збільшення інформації про поточну точку траєкторії спуску та локальні властивості функції.
3.	Аналіз останніх досліджень
Дослідження чисельних методів мінімізації функцій багатьох змінних має довгу історію. Найбільш очевидним серед чисельних методів мінімізації функцій багатьох змінних є метод градієнтного спуску. Його ідея запропонована ще А. Коші. Вивченню цього методу присвячена робота Л. В. Канторовича [6], в якій отримано оцінку швидкості збіжності при мінімізації квадратичної функції на основі інформації про обумовленість відповідної матриці других похідних.
Значний успіх у конструюванні та дослідженні різних аспектів чисельних методів безумовної мінімізації припадає на 70-ті роки ХХ ст., коли було побудовано велику кількість таких методів та закладено основи їх дослідження на швидкодію та обчислювальну складність. Пізніше з’явились нові принципи конструювання оптимізаційних процедур, основані на емерджентних принципах, зокрема, генетичні методи, які знайшли своє застосування при розв’язуванні великої кількості прикладних задач. В той же час проблема застосування адекватного чисельного методу оптимі-зації для вирішення прикладної проблеми залишається відкритою. Це пов’язане із особливістю побудови обчислювальної процедури, яка володіє властивістю «жадібності». Це проявляється в тому, що кожне наступне наближення будується на основі невеликої кількості інформації про властивості функції. Тому можна казати, що пошук наступної точки у просторі аргументів здійснюється в умовах часткової невизначеності. Збільшення ефективності методу у разі вирішення практичної задачі та прискорення збіжності обчислювальних процедур має сенс здійснювати на основі врахування властивостей функції як у черговій точці траєкторії спуску мінімізаційної послідовності, так і в її околі. Цю додаткову інформацію доцільно використати при побудові обчислювальних формул. Ще одним можливим способом покращення властивостей чисельних методів є введення спеціальних параметрів в такі обчислювальні формули. З допомогою таких параметрів можна уточняти напрямок спуску, а самі параметри знаходити з допомогою процедур, які узгоджують їх в процесі вирішення задачі.
Розробка методів мінімізації є відносно самостійною галуззю обчислювальної математики, яка має на своєму рахунку значні досягнення. Ефективне розв’язування задачі мінімізації функції дозволяє істотно просунутися при вирішення багатьох прикладних проблем аналізу даних, які у різний спосіб зводяться до оптимізаційних задач без обмежень. Основною причиною яка викликає зацікавленість методами мінімізації, є їх широке поширення в задачах аналізу даних, зокрема, при застосуванні нейронних мереж прямого поширення. Тривалий час, відтоді як градієнтний метод був запропонований та до 50-х років ХХ-го ст., мінімізація функцій виконувалась градієнтними методами, в яких послідовні наближення будують на основі лінійної апроксимації функції. Було запропоновано та ґрунтовно досліджено багато модифікацій градієнтних методів [7, 8, 9, 10]. Виявилось, що для всіх варіантів градієнтних методів швидкість збіжності невисока, тому застосування їх для вирішення складних завдань в більшості випадків є недоцільним. Дослідження Б. Поляка [11] показали, що швидкість збіжності градієнтного методу за умови спеціального вибору довжини його кроку, оцінюється збіжністю геометричної прогресії, знаменник якої залежить від властивостей функції, що мінімізують. До функцій, мінімізація яких градієнтними методами вважається неможливою, відносять так звані функції «типу яру». Деякі автори пропонували методи прискорення швидкості збіжності таких функцій, які певною мірою були б позбавлені вказаного недоліку. Загальна ідея побудови таких методів була висловлена І. Гельфандом та І.Цетліним [12], які запропонували після отримання
інформації про поведінку функції на двох послідовних ітераціях, зробити великий крок випадкової довжини уздовж «яру». Повторення у такий спосіб вказаної процедури надає можливість отримати точку мінімуму або вийти з області «яру».
4.	Цілі статті
У пропонованій статті побудовано та досліджено модифікації методів мінімізації функцій багатьох змінних, які стосуються двох класів методів - градієнтних та двоїстих напрямків, та їх модифікацій, зокрема, градієнтно-параметричного методу та модифікованого методу двоїстих напрямків, відповідно. Запропоновані формули використовують обчислення лише перших похідних функцій. Основна відмінність пропонованих модифікацій методів полягає у використанні додаткової інформації у кожній точці траєкторії спуску. Такий підхід до побудови напрямку спуску дозволив ввести в обчислювальні формули параметри, з допомогою яких виникає можливість регулювати цей напрямок спуску із врахуванням локальних особливостей функцій. Також це дозволяє забезпечити швидкість збіжності до точки мінімуму.
5.	Основний матеріал
Градієнтно-параметричний метод. Розглянемо алгоритм мінімізації опуклої достатньо гладкої функції п змінних f (х), який задано рекурентними форму-
хк = хк-М'(хк) к = хк+і = хк -Рк (9^'(хк) + (!-ек)f'(хк)^ 0 <9^1
(1)
де Xк , Рк - скалярні множники, причому вибір Xк здійснюється з умови
f (хк -Х^'(хк )) = піі^ (хк	'(хк)).
X
(2)
Для вибору параметра Рк можна використати будь-який алгоритм, який забезпечить виконання співвід-
ношення
f (хк - РкРк)- f (хк )^ -єРк № ,Рк ) 0 < є< °Д Рк = еЛхк) + ( 1 -ек) ^(хк).
(3)
Нескладно переконатись, що у разі вибору 9к = 1 та Xк із співвідношення (2) буде отримано метод найшвидшого спуску.
Використовуємо алгоритм (1) для мінімізації квадратичної функції
f(х) = 0.5(Ах,х),
(4)
де А - симетрична, додатно визначена матриця, для якої виконані умови
ш||уЦ 2<(Ау,у)< М||У 2, 0 < т < М, у єЕ".
Тоді
З
2. || рк|| ^0 при к ^
Нехай для рк-і та єк-і виконано співвідношення:
Рк-і = хк-0,5і	х к—0.5і—1,
^(Хк ) = Axk,
Рк = ЄкАхк + (1 - Єк ) А (Хк - ХкАхк ) = А (Хк + ХкЄАхк ) ,
хк=1 ІАхкІІ2 (А2хк,Ахк)1, к=°д...
Теорема 1. Якщо функція і(х) обмежена знизу, її градієнт задовольняє умову Ліпшиця |і'(х)-І '(у)||< К|| х - у || для будь-яких х,у єЕ", вибір для непарного п, або параметрів Xк та Рк здійснюється з умов (2) та (3)
відповідно, то для процесу (1) при к ^ 0 виконуєть-	ІРк—і = хк—0,5(і—1) - хк—0.5(і—1)т
ся ||ік||^0 незалежно від вибору початкової точки	|єк—і = і(хк—05(і—1) + рк—і) — і'(хк—05(і—1)),
к-і = f (хк-0.5і-1 + Рк-і ) f (хк-0.5і-1 ),
де і = 0,2,...,п-2 для парного п або і = 0,2,...,п-1
Теорема 2. Нехай для мінімізації квадратичної функції (4), що має симетричну додатно визначену матрицю А застосовано метод (1)-(2) Тоді для швидкості збіжності мінімізуючої послідовності { хк} виконується оцінка
[Мк
=*/—п qi
V т і=о
хк+1І<\ —Пqi ІКЦ, qi <1.
У формулі (5) позначено
N (9) =
т 1,к = хк
92
1 - 2Т 1,к (1 -9к ) + Т2,к (1 -9к ) ||\2
IIа хк ІІАхкІ
, У 2,к = Хк
(АЧ,АЧ ) ||Ахк|| 2
хк = хк	акАк-1f (хк ) ,
і -1
хк+1 = хк-«кАк^'(хк) , к = 0,1,2,... ,
АкРк-і =Єkкi, і = 0,1,2,...,п - 1.
де і = 1,3,...,п — 1 для парного п або і = 1,3,...,п — 2 для п непарного.
Нехай для матриці других похідних і"(х) функції і (х) виконана умова Ліпшица
||Р(х)-f "(у)||< к||х - у, х,у єЕп,
(5)
а також виконується
ті УІГ < ^" (х) у, у) < МІ УІГ , 0 < т < М .
(10)
(11)
Модифікований метод двоїстих напрямків. Розглянемо модифікацію методу двоїстих напрямів [5] мінімізації сильно опуклої достатньо гладкої функції п змінних і (х), обчислювальну формулу якого запишемо у вигляді
(6)
(7)
а вибір параметрів а к та ак здійснюється з умов і(хк + акРк) = ш±іпі(хк + аРк),
і(хк + акРк) = шІпі(хк + аРк).	(8)
Основна відмінність пропонованої модифікації від базового методу двоїстих напрямків полягає у способі обчислення матриці А^.
Матриця Ак визначається системою рівнянь
(9)
Тут Рк—і послідовність векторів, для якої виконані умови:
1. Якщо	Дк	- визначник зі стовпцями
Р^ІР^І—1,Рк—Л Рк—11—1,-.,Рк—п+Л Рк—п+11—1, то при будь-якому к виконується нерівність |Дк| > С >є , є - довільне мале додатне число.
Тоді можна отримати оцінки швидкості збіжності методу, які сформульовані у наступній теоремі.
Теорема 3. Якщо для мінімізації функції і(х) для якої виконано умови (9)-(10) використовується метод (6)-(7) у якому вибір параметра ак з умови (8), то послідовність {хк} збігається до точки мінімуму із швидкістю, яка визначена оцінкою
II хк+1 — х* || < С || хк—0,5(п—2) — х* |[\\ хк — х* |її хк—0,5п — х*||
для п парного,
I хк+1 - хА\ < С хк-0,5(п+1) - х* И Xkк0,5(nкl) - х* \І хк - х*
для п непарного.
Систему (9) можна так записати у матричному вигляді А-1 = ЯкЕ-1, де Ек , Як - матриці, стовпцями яких є відповідно вектори £к-і , рк-і. Для побудови матриці А-1 необхідно обчислити матрицю Е-1, рядками якої будуть вектори, які утворюють базис sk,skk1,...,skkn+1 , що двоїстий до базису £к, єк-1,..., єк-п+1.
Кожна з матриць Ек , к = 1,2,... відрізняться від сусідніх зліва та справа двома стовпцями. Тому побудову базису sk,skk1,...,skkn+1 можна здійснювати рекурентними співвідношеннями. Покажемо спосіб їх побудови.
Нехай матриця Ек вже побудована, тобто відомий базис sk,skk1,...,skkn+1. Побудуємо систему векторів
^Л+р.-Л-п+з у такий спосіб
\+2 = ^	(^к-п+1, Єк+1^к-п+2 - (skkn+2, £к+1 ^к-п+1) ,
Sk+1 = ^	((skkn+2, Єк+2)skkn+1 - ^к-п+1, Єк+2)skkn+2) ,
де
^ = (skkn+2, Єк+2 )(^-п+1, £к+1 ) - (skkn+2, £к+1 )(^-п+1, Єк+2 ) ,
\+1-j = Sk+1kj - (^к+Н, £к+1 ) Sk+1 + ^к+1-, Єк+2 ) Sk+2 )
для ] = 1,...,п- 2 ,
х
0
для ] = 2,3,...,п-1. Якщо врахувати, що
А-1 = £Рк_іЄ'
т
:-і°к-і
(12)
Запропоновані модифікації методів використовують лише перші похідні функцій, що мінімізуються. Побудовані формули дозволяють адаптувати обчислювальний процес до особливостей функції шляхом вибору відповідних значень параметрів, так і використанням додаткової інформації про функцію у кожній точці. Запропоновані модифікації методів можуть дозволити значною мірою послабити проблему уповільнення збіжності градієнтних методів у разі мінімізації певних класів функцій. Широке застосування методу зворотного поширення похибки, який базується на методі градієнтного спуску, в задачах навчання нейромереж, вимагає побудови саме таких методів. Подальші дослідження запропонованих методів можуть бути проведені в напрямку отримання практичних рекомендацій щодо
АЛ-1 = А-1 ((рк+2 -Лк1єк+2)(аіЛ_п+2 -аіА_п+1 )Т - (рк+1 -Лк1єк+1)(а218к_п+2 -а2А_п+1 )Т) заст°су вання	запроп<ш°ваних
то формули (6) та (7) набудуть вигляду
хк+і = хк-«кІ^к-іА)Рк-і , і=0
Хк = Хк-«кі^к-мА)Рк-і-2 .
На основі виразу (12) можна отримати таку рекурентну формулу Лк+1 = А-1 +АЛ-1 для обчислення
Л-1, де
А	апа22	а2іаі2 ,
аі1	(зк-п+1, Єк+1 ) , аі2	(зк-п+2, Єк+1 ) , а21 = (вк-п+1, Єк+2 ) ,
а22 = (зк-п+2, Єк+2 ) .
методів та оцінювання діапазону значень параметрів, які забезпечать стабільні результати у разі розв’язування прикладних задач із застосуванням нейронних мереж, а також широкому обчислювальному експерименті із застосування запропонованих методів.
Література
1.	Нікольський Ю.В. Невизначеність та надлишковість даних у моделі процесу їх аналізу / Ю.В.Нікольський // Східно-Європейський журнал передових технологій. - Харків, 2009. - № 6/2 (42). - С. 27-31.
2.	Хайкин С. Нейронные сети: полный курс/ Хайкин С. - М.: Издательский дом «Вильямс», 2006. - 1104 с.
3.	Нікольський Ю.В. Генетичні алгоритми в екстремальних задачах/ Ю.В.Нікольський, Ю.М. Щербина // Вісн. Львів. ун-ту. Сер.: Прикладна математика та інформатика. - 2000. - Вип. 2 . - С. 191-208.
4.	Режим доступу: - http://ru.wikipedia.org/wiki/Алгоритм_Левенберга_—_Марквардта.
5.	Пшеничный Б.Н. Численные методы в экстремальных задачах / Пшеничный Б.Н., Данилин Ю.М. - М.: Наука, 1975. - 320 с.
6.	Канторович Л.В. О методе наискорейшего спуска/ Канторович Л.В.// дан СССР. - 1947. - Вип.56. - С. 235-236.
7.	Бирман М.Ш. Некоторые оценки метода скорейшего спуска/ Бирман М.Ш // УМН. - 1950. - Вип. 5, №3. - С. 152-155.
8.	Алексеев В. И. Градиентный метод поиска экстремума с вычислением коэффициента овражности / Алексеев В. И. // Автом. и вычисл. техника. - 1976. - №3. - С.43-46.
9.	Фридман В.М. О сходимости методов типа наискорейшего спуска / Фридман В.М. // УМН. - 1962. - Вип.17, №3. - С.201-204.
10.	Сафро В.М. О скорости сходимости некоторых градиентных методов/ Сафро В.М. // ЖВМ и МФ. - 1976. - №2, вип.16. - С. 496-499.
11.	Поляк Б.Т. Градиентные методы минимизации функционалов / Поляк Б.Т. // ЖВМ и МФ. - 196З. - Вип.3, №4. - С.643-654.
12.	Гельфанд И.М. Принцип нелокального поиска в задачах автоматической оптимизации / Гельфанд И.М., Цетлин М.Л. // ДАН СССР. - 1961. - Вып. 137. - С.295-298.
3