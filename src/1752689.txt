

	 Пример одного цикла обучения НС
	 

 
Читайте также:Break; - прерывание цикла.II. 8.4. Развитие речи в процессе обученияII. Иллюстративные аргументы – примеры.II. Как в процессе обучения приобрести практический опыт?II. Ответы на вопросы исходного уровня знанийIII. Информационное обеспечение обученияIII. Примеры физиологического строения животныхIV. Пример решения контрольной работыVI. ПРИМЕРНАЯ ТЕМАТИКА ПИСЬМЕННЫХ РАБОТVI. Примерный перечень вопросов для самопроверкиVII. Примеры для самостоятельного решения (8 мин.).А) Очная форма обучения






В качестве примера можно взять обучающую выборку:
 х1 х2 у
 1 0 2
 2 1 6
 4 2 16
 Здесь х1 и х2 – входные параметры НС, у – желаемый выходной параметр.
 Поскольку у аппроксимируемой функции два входных параметра и один выходной, то выбирается НС с двумя нейронами во входном слое и одним в выходном. Количество нейронов скрытого слоя примем равным двум. То есть формируется сеть вида 2-2-1 (см. рисунок 5).
        
 Рисунок 5 – Пример НС
  
 В качестве функции активации выбирается сигмоидная функция с коэффициентом a = 1. Начальные значения весов синаптических связей принимаются равными 0,5:
 w211 = w221 = w212 = w222 = w311 = w312 = 0,5.
 Поскольку исходные значения х1 , х2 и у не лежат в пределах [0, 1], их необходимо пронормировать, поделив, например, х1 на 4, х2 на 2, а у на 16. В результате получена нормированная выборка:
 х1 х2 у
 0,25 0 0,125
 0,5 0,5 0,375
 1 1 1
 Скорость обучения принимается равной n = 0,2.
   
    После подготовки можно приступать к обучению.
 Шаг 1. На входы НС подается первый вектор входных параметров: х1 = 0,25 и х2 = 0. При этом ужел = 0,125.
 Выходы нейронов входного слоя: Y11 = 0,25, Y12 = 0.
 Для скрытого слоя:
 U21 = w211*Y11 + w212*Y12 = 0,5*0,25 + 0,5*0 = 0,125;
 U22 = w221*Y11 + w222*Y12 = 0,5*0,25 + 0,5*0 = 0,125;
 Y21 = 1 / (1 + exp(-a*U21)) = 1 / (1 + exp(-0,125)) = 0,5312;
 Y22 = 1 / (1 + exp(-a*U22)) = 1 / (1 + exp(-0,125)) = 0,5312.
 Для выходного слоя:
 U31 = w311*Y21 + w312*Y22 = 0,5*0б5312 + 0,5*0б5312 = 0,5312;
 Y31 = 1 / (1 + exp(-a*U31)) = 1 / (1 + exp(-0,5312)) = 0,6298.
 Шаг 2. Величина градиента для выходного нейрона:
 EI31 = (Y31 – yжел)*Y31*(1 – Y31) = (0,6298 – 0,125)*0,6298*(1 - 0,6298) = 0,1177.
 Шаг 3. Величины градиентов для скрытого слоя:
 EI21 = Y21*(1 – Y21)*[EI31*w311] = 0,5312*(1 – 0,5312)*0,1177*0,5 = 0,01466,
 EI22 = Y22*(1 – Y22)*[EI31*w312] = 0,5312*(1 – 0,5312)*0,1177*0,5 = 0,01466.
 Шаг 4. Коррекция весов синапсов:
 w211 = w211 - n*Y11*EI21 = 0,5 – 0,2*0,25*0,01466 = 0,4993,
 w221 = w221 - n*Y11*EI22 = 0,5 – 0,2*0,25*0,01466 = 0,4993,
 w212 = w212 - n*Y12*EI21 = 0,5 – 0,2*0*0,01466 = 0,5,
 w222 = w222 - n*Y12*EI22 = 0,5 – 0,2*0*0,01466 = 0,5,

 w311 = w311 - n*Y21*EI31 = 0,5 – 0,2*0,5312*0,1177 = 0,4875,
 w312 = w312 - n*Y22*EI31 = 0,5 – 0,2*0,5312*0,1177 = 0,4875.
 Если при полученных весах на вход НС подать тот же вектор входных параметров, то на выходе будет у = 0,6267, что уже ближе к желаемому ужел = 0,125. То есть данный цикл обучения приблизил ответ НС к желаемому на величину Dу = 0,6298 – 0,6267 = 0,0031.
 Поскольку обучающая выборка не закончилась, то шаги 1 – 4 повторяются аналогично для следующего вектора входных параметров.
  



Правило обратного распространения  | следующая ==> Экспериментальная часть
Дата добавления: 2015-10-22; просмотров: 19; Опубликованный материал нарушает авторские права?.

Не нашли то, что искали? Воспользуйтесь поиском:
Не хотите "париться"? закажите платную работу!
	     
      